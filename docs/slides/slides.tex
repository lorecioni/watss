\documentclass{beamer}
\usepackage{xcolor}
\usepackage{mathtools}
\usetheme[titlepagelogo=firenze,% Logo for the first page
		  language=italian,
		  bullet=circle,
		  color=blue,
         ]{TorinoTh}
\usepackage[beamer,customcolors]{hf-tikz}
\definecolor{UniBlue}{RGB}{83,121,170}
\setbeamercolor{block title}{use=structure,fg=white,bg=UniBlue}
\setbeamercolor{block body}{use=structure,fg=black,bg=white}

\newcommand*{\bfrac}[2]{\genfrac{}{}{0pt}{1}{#1}{#2}}

\author{}
\rel{{\normalsize \emph{Visual and Multimedia Recognition}}\\\vspace{0.3cm}Lorenzo Cioni}
\title{\huge Improving WATSS web application with Computer Vision techniques}
\date{}

\begin{document}

\titlepageframe

\begin{tframe}{Introduction}

WATSS, \textbf{Web Annotation Tool for Surveillance Scenarios}, is a web-based annotation tool developed for annotating dataset in surveillance systems [1].

\vspace{0.3cm}

\textbf{Main goal}: improve WATSS with some \textbf{Computer Vision approaches}, in order to make easy for users to use this tool and make the annotation process more \emph{automatic}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\textwidth]{images/frame.jpg}
\end{center}
\label{fig:mainframe}
\end{figure}

\end{tframe}

\begin{tframe}{Comparative analysis of annotation tools}
\textbf{LabelMe }[4]
\begin{columns}[t] % align columns
\begin{column}{.5\textwidth}
\begin{itemize}
\item \textbf{Web-based} tool, also released as mobile application
\vspace{0.2cm}
\item Annotate scenes with \textbf{polygonal shapes}
\vspace{0.2cm}
\item \textbf{Nested} objects and \textbf{occlusion} annotations
\vspace{0.2cm}
\item \emph{Zoom in} and \emph{out} of the scene
\end{itemize}
\end{column}%
\begin{column}{.3\textwidth}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/labelme.jpg}
\end{figure}
\end{column}%
\end{columns}
\end{tframe}


\begin{tframe}{Comparative analysis of annotation tools}
\textbf{ViPER-GT} 
\begin{columns}[t] % align columns
\begin{column}{.5\textwidth}
\begin{itemize}
\item \textbf{Java application} tool
\vspace{0.2cm}
\item Annotate scenes with \textbf{geometrical shapes}
\vspace{0.2cm}
\item \textbf{Timeline} and \textbf{annotation highlighting} on time change
\vspace{0.2cm}
\item Linear \textbf{interpolation} between annotations
\vspace{0.2cm}
\item \emph{Zoom in} and \emph{out} of the scene
\end{itemize}
\end{column}%
\begin{column}{.3\textwidth}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/vipergt.jpg}
\end{figure}
\end{column}%
\end{columns}
\end{tframe}

\begin{tframe}{Comparative analysis of annotation tools}
\textbf{VATIC} [3] 
\begin{columns}[t] % align columns
\begin{column}{.45\textwidth}
\begin{itemize}
\item \textbf{Online} tool
\vspace{0.1cm}
\item Developed for \textbf{object detection}
\vspace{0.1cm}
\item \textbf{Crowd-sourcing} to Amazon's \emph{Mechanical Turk}
\vspace{0.1cm}
\item Multiple \textbf{plugins}: \emph{object tracking}, \emph{sentence annotation}, etc.
\end{itemize}
\end{column}%
\begin{column}{.3\textwidth}
\begin{figure}[h]
\centering
\includegraphics[width=1.2\textwidth]{images/vatic.jpg}
\end{figure}
\end{column}%
\end{columns}
\end{tframe}

\begin{tframe}{Comparative analysis of annotation tools}
\textbf{WATSS} 
\begin{columns}[t] % align columns
\begin{column}{.45\textwidth}
\begin{itemize}
\item \textbf{Web-based} tool
\vspace{0.1cm}
\item Annotation with bounding box
\vspace{0.1cm}
\item \textbf{Occlusion} area
\vspace{0.1cm}
\item Coarse \textbf{gaze} estimation
\vspace{0.1cm}
\item \textbf{Groups} and \textbf{POI} under observation
\vspace{0.1cm}
\item \textbf{Multiple cameras} manager
\end{itemize}
\end{column}%
\begin{column}{.3\textwidth}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/gaze.jpg}
\end{figure}
\end{column}%
\end{columns}
\end{tframe}


\begin{tframe}{Improvements}
\begin{itemize}
\vspace{0.3cm}
\item \textbf{User interface} renovation
\vspace{0.2cm}
\item Simpler annotation making and editing
\vspace{0.2cm}
\item Video \textbf{timeline} for annotations
\vspace{0.2cm}
\item Automatic \textbf{proposals} generation
\vspace{0.2cm}
\item \textbf{Scene geometry-based} enhancement
\vspace{0.2cm}
\item Easy \textbf{setup} process
\end{itemize}
\end{tframe}

\begin{tframe}{User interface renovation}
The \textbf{old} WATSS user interface
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/watss_old.jpg}
\end{figure}
\end{tframe}

\begin{tframe}{User interface renovation}
The \textbf{new} WATSS user interface
\begin{figure}[h]
\centering
\includegraphics[scale=0.22]{images/watss-gui.jpg}
\end{figure}
\end{tframe}

\begin{tframe}{Video timeline}
In the video \textbf{timeline} all the video frames are shown, coloring the ones with at least one annotated person. 
\vspace{0.2cm}
\begin{figure}[h]
\centering
\includegraphics[scale=0.22]{images/timeline.jpg}
\end{figure}
\vspace{0.2cm}
Selecting a person in the list makes the timeline display its \textbf{history} highlighting frames where it is present. It is possible to navigate video frames by clicking on it.
\end{tframe}

\begin{tframe}{Proposals generation}
It is possible to generate \textbf{proposals} for a person in some selected frames based on previous annotation of it using timeline: just click and drag the highlighted annotation.

\vspace{0.3cm}

Proposals generation is based on the combination of three different techniques:
\vspace{0.4cm}
\begin{itemize}
\item \textbf{Motion detection} using a \emph{background subtractor}
\vspace{0.2cm}
\item \textbf{Pedestrian detection} using \emph{HOG descriptors}
\vspace{0.2cm}
\item \textbf{Kalman filter} for the \emph{motion estimation}
\end{itemize}
\end{tframe}


\begin{tframe}{Motion detection}
Motion detection is based on a \textbf{background subtraction} method: moving objects are detected performing a subtraction between the current frame and a background model of the current scene, obtaining a \textbf{foreground mask}.

\vspace{0.3cm}
Each pixel of a frame is modeled as a \textbf{Mixture of Gaussians} and those which correspond to background colors are selected according to variance and persistence.

Pixel values that do not fit the background distributions are considered part of the foreground.

\vspace{0.2cm}
Background modeling consists of two main steps:
\begin{itemize}
\item \textbf{Background initialization}: background model evaluation.
\item \textbf{Background update}: background model is adapted to possible changes in the scene.
\end{itemize}
\end{tframe}


\begin{tframe}{Motion detection}
Using the foreground mask, a set of \textbf{detections} are extracted based on contours.
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/frame_mog.jpg}
\end{figure}

Given the previous frame person bounding box, those that do not \emph{overlap} or are \emph{inconsistent} with its dimensions are discarded.
\end{tframe}

\begin{tframe}{Pedestrian detector}
The used \emph{pedestrian detection} technique is based on \textbf{Histogram of Oriented Gradients} and SVM classifier.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/hog_detector.jpg}
\end{figure}

As in the previous case, detections are filtered according to the person history in scene.

\vspace{0.2cm}
\hfill {\tiny Figure from Suleiman, A., Sze, V. J \emph{Signal Process Systems} (2016)}
\end{tframe}


\begin{tframe}{Kalman filter}

A \textbf{Kalman filter} is an optimal estimator used for following state estimations based on a set of previous observations.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{images/kalmaneq.jpg}
\end{figure}
A single state in the system considers the \textbf{coordinates} $(x, y)$ \textbf{of the person} in the current scene, using motion and pedestrian detection for updating the measure.
\end{tframe}


\begin{tframe}{Proposals generation}
A proposal for a generic frame is the result of the combination of the above descripted methods. Each resulted bounding box is compared with the previous frame annotation in order to evaluate a \textbf{score}:
\vspace{0.2cm}

$$score(r) = \frac{intersection(r, p)}{union(r, p)}$$

\vspace{0.3cm}

where $r$ is a resulting bounding box (\emph{i.e. the output of the motion detector}) and $p$ is the bounding box of the previous frame. 

\vspace{0.3cm}

If the motion or the pedestrian detector fails, then the \emph{Kalman filter prediction} is used as the proposal.
\end{tframe}


\begin{tframe}{Proposals generation example}

\begin{figure}[h]
\centering
\includegraphics[width=.3\textwidth]{images/original.jpg}\quad
\includegraphics[width=.3\textwidth]{images/motion.jpg}\quad
\includegraphics[width=.3\textwidth]{images/pedestrian.jpg}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=.45\textwidth]{images/prediction1.jpg}\quad
\includegraphics[width=.45\textwidth]{images/prediction2.jpg}\quad
\end{figure}

\end{tframe}


\begin{tframe}{Scene geometry}
In the annotation insertion step, it is possible to use the\textbf{ scene geometry} knowledge in order to generate proposals based on the pointer position.

\vspace{0.5cm}
Requirements:
\vspace{0.3cm}
\begin{itemize}
\item \textbf{Static} cameras
\vspace{0.2cm}
\item Camera \textbf{calibration parameters}
\vspace{0.2cm}
\begin{itemize}
\item \emph{Intrinsic} parameters, $\textbf{K}$
\item \emph{Estrinsic} parameters, rotation $\textbf{R}$ and translation $\textbf{t}$
\item A \emph{cross-ratio} \textbf{$\mu$}, being projective invariant
\end{itemize}
\end{itemize}
\end{tframe}

\begin{tframe}{Camera calibration}
Given $\textbf{X} = (X, Y, Z, 1)$ as coordinates in \textbf{world}, and $\textbf{x} = (x, y, 1)$ as coordinates in \textbf{scene}:
$$\textbf{x} = K [R | t] X$$
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{images/camera.jpg}
\end{figure}
\end{tframe}

\begin{tframe}{Human height estimation}
From the camera parameters, it is possible to evaluate the \textbf{vanishing line} $\textbf{l}$ and the \textbf{vanishing point} $\textbf{v}$


\begin{columns}[t] % align columns
\begin{column}{.45\textwidth}
$$l = P * [0, 0, 1]'$$
$$v = ((K')^{-1} * K^{-1}) * l$$
$$W = I + (\frac{1}{(1 - \mu)} - 1) * \frac{v * l'}{v' * l}$$

\vspace{0.2cm}
Given the head position $head = (x, y, 1)$ and $\textbf{W}$:
$$feet = W^{-1} * head$$
$$height = |head_y - feet_y|$$

\end{column}%
\begin{column}{.5\textwidth}
\begin{figure}[h]
\centering
\includegraphics[width=1.1\textwidth]{images/height_calib.jpg}
\end{figure}
\end{column}%
\end{columns}


\end{tframe}


\begin{tframe}{Demo}
Time for a \textbf{demo}!
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/demo.jpg}
\end{figure}
\end{tframe}


\begin{tframe}{Conclusions}

WATSS application has been improved with \textbf{new features} and a renewed \textbf{user interface}.
\vspace{0.2cm}

Some features are based on Computer Vision techniques, as automatic annotation proposals generation and scene geometry-based annotation insertion.
\vspace{0.4cm}

\textbf{Future developments}:
\vspace{0.2cm}
\begin{itemize}
\item Use scene geometry in proposals generation
\vspace{0.2cm}
\item Using different people detectors, e.g. an \textbf{upper body detector}, for more accurate proposals
\vspace{0.2cm}
\item Different types of \textbf{annotation shapes}, e.g circle, ellipsis, polygon etc.
\end{itemize}

\end{tframe}


\begin{tframe}{References}

\begin{scriptsize}

[1] F. Bartoli, L. Seidenari, G. Lisanti, S. Karaman, and A. Del Bimbo. \emph{Watts: A web annotation tool for surveillance scenarios}. In \emph{Proceedings of the 23rd ACM International Conference on Multimedia, MM ’15}, pages 701–704, New York, NY, USA, 2015. ACM.

\vspace{0.1in}

[2] F. Bartoli, G. Lisanti, L. Seidenari, S. Karaman, and A. Del Bimbo. \emph{Museumvisitors: A dataset for pedestrian and group detection, gaze estimation and behavior understanding}. In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops}, pages 19–27, 2015.

\vspace{0.1in}

[3] C. Vondrick, D. Patterson, and D. Ramanan. \emph{Efficiently scaling up crowdsourced video annotation}. Int. J. Comput. Vision, 101(1):184–204, Jan. 2013.

\vspace{0.1in}

[4] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. \emph{Labelme: A database and web-based tool for image annotation}. Int. J. Comput. Vision, 77(1-3):157–173, May 2008.


\vspace{0.1in}

[5] A. Del Bimbo, G. Lisanti, and F. Pernici. \emph{Scale invariant 3d multi-person tracking using a base set of bundle adjusted visual landmarks}. In Computer Vision Workshops (ICCV Work- shops), 2009 IEEE 12th International Conference on, pages 1121–1128. IEEE, 2009.

\end{scriptsize}

\end{tframe}

\end{document}